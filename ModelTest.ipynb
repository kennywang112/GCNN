{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68255679-c372-4ffc-a12b-e4b65248873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0441f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from utils.utils_train_model import *\n",
    "from utils.utils_preprocess import *\n",
    "from models import *\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "device = (\n",
    "    \"mps\" \n",
    "    if torch.backends.mps.is_available() \n",
    "    else \"cuda\" \n",
    "    if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "data_dict_path = './output_data/data_dict.pth'\n",
    "data_dict = torch.load(data_dict_path)\n",
    "\n",
    "print(\"Processing data\")\n",
    "print([len(data_dict[f'{i}']) for i in range(1, 8)])\n",
    "\n",
    "data_list = sum([data_dict[str(i)] for i in range(1, 8)], [])\n",
    "total_size = len(data_list)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_data, val_data = random_split(data_list, [train_size, val_size])\n",
    "\n",
    "# Step 2: Create DataLoader for training and validation\n",
    "batch_size = 64\n",
    "train_loader = GeoDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = GeoDataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 3: Model setup\n",
    "# Some of the datas don't have adj matrix\n",
    "num_node_features = next(data.x.shape[1] for data in data_list if data.x is not None)\n",
    "\n",
    "hidden_channels = 64\n",
    "model_dict = get_model_list(device, num_node_features, hidden_channels)\n",
    "# model = Net_Alex(num_node_features, hidden_channels).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea05aa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "ðŸƒ View run adventurous-gnat-860 at: http://127.0.0.1:5000/#/experiments/0/runs/0e90da1598ad41e8bf1b1f8175c8c8b2\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 32\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m _, pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     35\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m batch_y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Start training ...')\n",
    "for i, (model_name, model) in enumerate(model_dict.items()):\n",
    "    params = {\n",
    "    \"model\": model_name,\n",
    "    \"num_epochs\": 25,\n",
    "    \"lr\":0.001,\n",
    "    }\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    cumulative_preds = []\n",
    "    cumulative_labels = []\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for batch in train_loader:\n",
    "                batch  = batch.to(device)\n",
    "                x = batch.x\n",
    "                edge_index = batch.edge_index\n",
    "                edge_weight = batch.edge_weight\n",
    "                image_features = batch.image_features\n",
    "                batch_y = batch.y\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 2 == 0:\n",
    "                    out = model(image_features)\n",
    "                    loss = criterion(out, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    _, pred = out.max(dim=1)\n",
    "                    correct += (pred == batch_y).sum().item()\n",
    "                else:\n",
    "                    GNN_output, CNN_output = model(x, edge_index, edge_weight, batch.batch, image_features)\n",
    "                    if GNN_output is not None:\n",
    "                        loss_GNN = criterion(GNN_output, batch_y)\n",
    "                        loss_AlexNet = criterion(CNN_output, batch_y)\n",
    "                        loss = (loss_GNN + loss_AlexNet) / 2\n",
    "                        loss.backward()\n",
    "                        _, pred = GNN_output.max(dim=1)\n",
    "                        correct += (pred == batch_y).sum().item()\n",
    "                    else:\n",
    "                        loss = criterion(CNN_output, batch_y)\n",
    "                        loss.backward()\n",
    "                        _, pred = CNN_output.max(dim=1)\n",
    "                        correct += (pred == batch_y).sum().item()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_accuracy = correct / len(train_data)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            epoch_preds = []\n",
    "            epoch_labels = []\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    x = batch.x\n",
    "                    edge_index = batch.edge_index\n",
    "                    edge_weight = batch.edge_weight\n",
    "                    image_features = batch.image_features\n",
    "                    batch_y = batch.y\n",
    "\n",
    "                    if i % 2 == 0:\n",
    "                        out = model(image_features)\n",
    "                        loss = criterion(out, batch_y)\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                        _, pred = out.max(dim=1)\n",
    "                        val_correct += (pred == batch_y).sum().item()\n",
    "                    else:\n",
    "                        GNN_output, AlexNet_output = model(x, edge_index, edge_weight, batch.batch, image_features)\n",
    "                        if GNN_output is not None:\n",
    "                            loss_GNN = criterion(GNN_output, batch_y)\n",
    "                            loss_AlexNet = criterion(AlexNet_output, batch_y)\n",
    "                            loss = (loss_GNN + loss_AlexNet) / 2\n",
    "                            _, pred = GNN_output.max(dim=1)\n",
    "                            val_correct += (pred == batch_y).sum().item()\n",
    "                        else:\n",
    "                            loss = criterion(AlexNet_output, batch_y)\n",
    "                            _, pred = AlexNet_output.max(dim=1)\n",
    "                            val_correct += (pred == batch_y).sum().item()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    epoch_preds.extend(pred.cpu().numpy())\n",
    "                    epoch_labels.extend(batch_y.cpu().numpy())\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = val_correct / len(val_data)\n",
    "            mlflow.log_metric('train_loss', train_loss, step=epoch)\n",
    "            mlflow.log_metric('train_accuracy', train_accuracy, step=epoch)\n",
    "            mlflow.log_metric('val_loss', avg_val_loss, step=epoch)\n",
    "            mlflow.log_metric('val_correct', val_accuracy, step=epoch)\n",
    "            print(f'Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "                f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Accumulate all predictions and labels across epochs\n",
    "            cumulative_preds.extend(epoch_preds)\n",
    "            cumulative_labels.extend(epoch_labels)\n",
    "    \n",
    "    save_path = f'./model/{model_name}.pth'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved to {save_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
