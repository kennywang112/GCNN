{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68255679-c372-4ffc-a12b-e4b65248873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangqiqian/opt/anaconda3/envs/Z_GCN_new/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.patches as mpatches\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "from models import *\n",
    "from utils.utils_preprocess import *\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0441f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from utils.utils_train_model import *\n",
    "from utils.utils_preprocess import *\n",
    "from models import *\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "device = (\n",
    "    \"mps\" \n",
    "    if torch.backends.mps.is_available() \n",
    "    else \"cuda\" \n",
    "    if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "data_dict_path = './output_data/data_dict.pth'\n",
    "data_dict = torch.load(data_dict_path)\n",
    "\n",
    "print(\"Processing data\")\n",
    "print([len(data_dict[f'{i}']) for i in range(1, 8)])\n",
    "\n",
    "data_list = sum([data_dict[str(i)] for i in range(1, 8)], [])\n",
    "total_size = len(data_list)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_data, val_data = random_split(data_list, [train_size, val_size])\n",
    "\n",
    "# Step 2: Create DataLoader for training and validation\n",
    "batch_size = 64\n",
    "train_loader = GeoDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = GeoDataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 3: Model setup\n",
    "# Some of the datas don't have adj matrix\n",
    "num_node_features = next(data.x.shape[1] for data in data_list if data.x is not None)\n",
    "\n",
    "hidden_channels = 64\n",
    "model_dict = get_model_list(device, num_node_features, hidden_channels)\n",
    "# model = Net_Alex(num_node_features, hidden_channels).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start training ...')\n",
    "for i, (model_name, model) in enumerate(model_dict.items()):\n",
    "    params = {\n",
    "    \"model\": model_name,\n",
    "    \"num_epochs\": 25,\n",
    "    \"lr\":0.001,\n",
    "    }\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    cumulative_preds = []\n",
    "    cumulative_labels = []\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for batch in train_loader:\n",
    "                batch  = batch.to(device)\n",
    "                x = batch.x\n",
    "                edge_index = batch.edge_index\n",
    "                edge_weight = batch.edge_weight\n",
    "                image_features = batch.image_features\n",
    "                batch_y = batch.y\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 2 == 0:\n",
    "                    out = model(image_features)\n",
    "                    loss = criterion(out, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    _, pred = out.max(dim=1)\n",
    "                    correct += (pred == batch_y).sum().item()\n",
    "                else:\n",
    "                    GNN_output, CNN_output = model(x, edge_index, edge_weight, batch.batch, image_features)\n",
    "                    if GNN_output is not None:\n",
    "                        loss_GNN = criterion(GNN_output, batch_y)\n",
    "                        loss_AlexNet = criterion(CNN_output, batch_y)\n",
    "                        loss = (loss_GNN + loss_AlexNet) / 2\n",
    "                        loss.backward()\n",
    "                        _, pred = GNN_output.max(dim=1)\n",
    "                        correct += (pred == batch_y).sum().item()\n",
    "                    else:\n",
    "                        loss = criterion(CNN_output, batch_y)\n",
    "                        loss.backward()\n",
    "                        _, pred = CNN_output.max(dim=1)\n",
    "                        correct += (pred == batch_y).sum().item()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_accuracy = correct / len(train_data)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            epoch_preds = []\n",
    "            epoch_labels = []\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    x = batch.x\n",
    "                    edge_index = batch.edge_index\n",
    "                    edge_weight = batch.edge_weight\n",
    "                    image_features = batch.image_features\n",
    "                    batch_y = batch.y\n",
    "\n",
    "                    if i % 2 == 0:\n",
    "                        out = model(image_features)\n",
    "                        loss = criterion(out, batch_y)\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                        _, pred = out.max(dim=1)\n",
    "                        val_correct += (pred == batch_y).sum().item()\n",
    "                    else:\n",
    "                        GNN_output, AlexNet_output = model(x, edge_index, edge_weight, batch.batch, image_features)\n",
    "                        if GNN_output is not None:\n",
    "                            loss_GNN = criterion(GNN_output, batch_y)\n",
    "                            loss_AlexNet = criterion(AlexNet_output, batch_y)\n",
    "                            loss = (loss_GNN + loss_AlexNet) / 2\n",
    "                            _, pred = GNN_output.max(dim=1)\n",
    "                            val_correct += (pred == batch_y).sum().item()\n",
    "                        else:\n",
    "                            loss = criterion(AlexNet_output, batch_y)\n",
    "                            _, pred = AlexNet_output.max(dim=1)\n",
    "                            val_correct += (pred == batch_y).sum().item()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    epoch_preds.extend(pred.cpu().numpy())\n",
    "                    epoch_labels.extend(batch_y.cpu().numpy())\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = val_correct / len(val_data)\n",
    "            mlflow.log_metric('train_loss', train_loss, step=epoch)\n",
    "            mlflow.log_metric('train_accuracy', train_accuracy, step=epoch)\n",
    "            mlflow.log_metric('val_loss', avg_val_loss, step=epoch)\n",
    "            mlflow.log_metric('val_correct', val_accuracy, step=epoch)\n",
    "            print(f'Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "                f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Accumulate all predictions and labels across epochs\n",
    "            cumulative_preds.extend(epoch_preds)\n",
    "            cumulative_labels.extend(epoch_labels)\n",
    "    \n",
    "    save_path = f'./model/{model_name}.pth'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39810ad",
   "metadata": {},
   "source": [
    "# Grad Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c020a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: \"Surprised\",\n",
    "    1: \"Fearful\",\n",
    "    2: \"Disgusted\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Sad\",\n",
    "    5: \"Angry\",\n",
    "    6: \"Neutral\"\n",
    "}\n",
    "\n",
    "device = (\n",
    "    \"mps\" \n",
    "    if torch.backends.mps.is_available() \n",
    "    else \"cuda\" \n",
    "    if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load data\n",
    "data_dict_path = './output_data/data_dict.pth'\n",
    "data_dict = torch.load(data_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6774c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = sum([data_dict[str(i)] for i in range(1, 8)], [])\n",
    "total_size = len(data_list)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_data, val_data = random_split(data_list, [train_size, val_size])\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "train_loader = GeoDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = GeoDataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_class = 7\n",
    "\n",
    "# Model setup\n",
    "num_node_features = next(data.x.shape[1] for data in data_list if data.x is not None)\n",
    "hidden_channels = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856bb6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model...\n",
      "Load model...\n",
      "Load model...\n",
      "Load model...\n",
      "Load model...\n",
      "Load model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load model...\")\n",
    "model_alexnet_gnn = Net_Alex(num_node_features, hidden_channels)\n",
    "model_alexnet_gnn.load_state_dict(torch.load('./model/model_Net_Alex.pth', map_location=torch.device(device)))\n",
    "model_alexnet_gnn.eval()\n",
    "model_alexnet_gnn.to(device)\n",
    "print(\"Load model...\")\n",
    "model_resnet_gnn = Net_ResNet18(num_node_features, hidden_channels)\n",
    "model_resnet_gnn.load_state_dict(torch.load('./model/model_Net_Resnet.pth', map_location=torch.device(device)))\n",
    "model_resnet_gnn.eval()\n",
    "model_resnet_gnn.to(device)\n",
    "print(\"Load model...\")\n",
    "model_alexnet = AlexNet_Only(num_class)\n",
    "model_alexnet.load_state_dict(torch.load('./model/model_alex_only.pth', map_location=torch.device(device)))\n",
    "model_alexnet.eval()\n",
    "model_alexnet.to(device)\n",
    "print(\"Load model...\")\n",
    "model_resnet = ResNet18_Only(num_class)\n",
    "model_resnet.load_state_dict(torch.load('./model/model_Resnet18_only.pth', map_location=torch.device(device)))\n",
    "model_resnet.eval()\n",
    "model_resnet.to(device)\n",
    "print(\"Load model...\")\n",
    "model_vgg = VGG16_Only(num_class)\n",
    "model_vgg.load_state_dict(torch.load('./model/model_VGG16_only.pth', map_location=torch.device(device)))\n",
    "model_vgg.eval()\n",
    "model_vgg.to(device)\n",
    "print(\"Load model...\")\n",
    "model_vgg_gnn = Net_VGG(num_node_features, hidden_channels)\n",
    "model_vgg_gnn.load_state_dict(torch.load('./model/model_Net_VGG.pth', map_location=torch.device(device)))\n",
    "model_vgg_gnn.eval()\n",
    "model_vgg_gnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25592511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pth = [\n",
    "#     'Image_data/DATASET/test/1/test_0002_aligned.jpg',\n",
    "#     'Image_data/DATASET/test/2/test_0274_aligned.jpg',\n",
    "#     'Image_data/DATASET/test/3/test_0007_aligned.jpg',\n",
    "#     'Image_data/DATASET/test/4/test_0003_aligned.jpg',\n",
    "#     'Image_data/DATASET/test/5/test_0001_aligned.jpg',\n",
    "#     'Image_data/DATASET/test/6/test_0017_aligned.jpg',\n",
    "#     'Image_data/DATASET/test/7/test_2389_aligned.jpg'\n",
    "# ]\n",
    "test_pth = [\n",
    "    'Image_data/DATASET/test/1/test_0004_aligned.jpg',\n",
    "    'Image_data/DATASET/test/2/test_0377_aligned.jpg',\n",
    "    'Image_data/DATASET/test/3/test_0011_aligned.jpg',\n",
    "    'Image_data/DATASET/test/4/test_0009_aligned.jpg',\n",
    "    'Image_data/DATASET/test/5/test_0005_aligned.jpg',\n",
    "    'Image_data/DATASET/test/6/test_0027_aligned.jpg',\n",
    "    'Image_data/DATASET/test/7/test_2390_aligned.jpg'\n",
    "]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2cf445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_Alex\n",
      "Net_ResNet18\n",
      "AlexNet_Only\n",
      "ResNet18_Only\n",
      "VGG16_Only\n",
      "Net_VGG\n"
     ]
    }
   ],
   "source": [
    "for model in [model_alexnet_gnn, model_resnet_gnn, model_alexnet, model_resnet, model_vgg, model_vgg_gnn]:\n",
    "\n",
    "    print(model.__class__.__name__)\n",
    "\n",
    "    for index, pth in enumerate(test_pth):\n",
    "\n",
    "        input_image = Image.open(pth)\n",
    "        input_tensor = transform(input_image).unsqueeze(0)  # Add batch dimension\n",
    "        rgb_img = np.array(input_image.resize((224, 224))) / 255.0\n",
    "\n",
    "        edge_weight = data_dict['1'][0].edge_weight\n",
    "        edge_index = data_dict['1'][0].edge_index\n",
    "        batch = val_loader.dataset[0]\n",
    "\n",
    "        if model.__class__.__name__ == 'Net_Alex':\n",
    "            wrapped_model = NetWrapper(model, edge_index, edge_weight, batch)\n",
    "            target_layers = [wrapped_model.model.alexnet.features[-1]]\n",
    "        elif model.__class__.__name__ == 'Net_ResNet18':\n",
    "            wrapped_model = NetWrapper(model, edge_index, edge_weight, batch)\n",
    "            target_layers = [wrapped_model.model.resnet.layer4[-1]]\n",
    "        elif model.__class__.__name__ == 'AlexNet_Only':\n",
    "           target_layers = [model.alexnet.features[-1]]\n",
    "           wrapped_model = model\n",
    "        elif model.__class__.__name__ == 'ResNet18_Only':\n",
    "            target_layers = [model.resnet.layer4[-1]]\n",
    "            wrapped_model = model\n",
    "        elif model.__class__.__name__ == 'VGG16_Only':\n",
    "            target_layers = [model.vgg16.features[-1]]\n",
    "            wrapped_model = model\n",
    "        elif model.__class__.__name__ == 'Net_VGG':\n",
    "            wrapped_model = NetWrapper(model, edge_index, edge_weight, batch)\n",
    "            target_layers = [wrapped_model.model.vgg16.features[-1]]\n",
    "\n",
    "\n",
    "        cam = GradCAM(model=wrapped_model, target_layers=target_layers)\n",
    "\n",
    "        grayscale_cam = cam(input_tensor=input_tensor)\n",
    "\n",
    "        visualization = show_cam_on_image(rgb_img, grayscale_cam[0, :], use_rgb=True)\n",
    "\n",
    "        try:\n",
    "            # save the plot\n",
    "            plt.imsave(f'static/GradCam/{model.__class__.__name__}_{index + 1}_2.png', visualization)\n",
    "        except FileNotFoundError:\n",
    "            os.mkdir('static/GradCam')\n",
    "            plt.imsave(f'static/GradCam/{model.__class__.__name__}_{index + 1}_2.png', visualization)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26039bae",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_alexnet_gnn, model_resnet_gnn, model_alexnet, model_resnet, model_vgg, model_vgg_gnn]:\n",
    "    print(model.__class__.__name__)\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            print(batch)\n",
    "            batch = batch.to(device)\n",
    "            x = batch.x\n",
    "            edge_index = batch.edge_index\n",
    "            edge_weight = batch.edge_weight\n",
    "            image_features = batch.image_features\n",
    "            batch_y = batch.y\n",
    "\n",
    "            if isinstance(model, Net_Alex) or isinstance(model, Net_ResNet18) or isinstance(model, Net_VGG):\n",
    "                # 处理 GNN + 图像模型\n",
    "                if x is not None and edge_index is not None:\n",
    "                    # 调用 forward 获取 GNN 和 AlexNet 的嵌入特征\n",
    "                    outputs, gnn_output = model(x, edge_index, edge_weight, batch.batch, image_features)\n",
    "                    combined_features = torch.cat((outputs, gnn_output), dim=1)\n",
    "                else:\n",
    "                    # 没有 GNN 数据时，直接用图像特征\n",
    "                    combined_features = model.alexnet(image_features)\n",
    "            else:\n",
    "                # 对于仅图像模型，直接获取图像特征\n",
    "                combined_features = model(image_features)\n",
    "\n",
    "            # 提取特征和标签\n",
    "            features_list.append(combined_features.cpu().numpy())\n",
    "            labels_list.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    # Combine features and labels\n",
    "    features = np.vstack(features_list)\n",
    "    labels = np.array(labels_list)\n",
    "\n",
    "    # Dimensionality reduction with UMAP\n",
    "    print(\"Performing UMAP dimensionality reduction...\")\n",
    "    reducer = umap.UMAP()\n",
    "    low_dim_embeddings = reducer.fit_transform(features)\n",
    "\n",
    "    # Visualization\n",
    "    print(\"Visualizing data...\")\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Create scatter plot\n",
    "    scatter = plt.scatter(\n",
    "        low_dim_embeddings[:, 0], \n",
    "        low_dim_embeddings[:, 1], \n",
    "        c=labels, \n",
    "        cmap='tab10', \n",
    "        s=40, \n",
    "        edgecolor='k', \n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    # Create a legend instead of a colorbar\n",
    "    unique_labels = np.unique(labels)\n",
    "    handles = [\n",
    "        mpatches.Patch(color=scatter.cmap(scatter.norm(label)), label=label_map[label]) \n",
    "        for label in unique_labels\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=handles, \n",
    "        title=\"Emotion Categories\", \n",
    "        loc='upper right', \n",
    "        fontsize=10, \n",
    "        title_fontsize=12\n",
    "    )\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.xlabel('Dimension 1', fontsize=14)\n",
    "    plt.ylabel('Dimension 2', fontsize=14)\n",
    "\n",
    "    # Customize grid and background\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.gca().set_facecolor('#f7f7f7')\n",
    "\n",
    "    # Save the plot\n",
    "    output_dir = './UMAPplots'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(f'{output_dir}/{model.__class__.__name__}.png', dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Z_GCN_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
