{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68255679-c372-4ffc-a12b-e4b65248873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangqiqian/opt/anaconda3/envs/Z_GCN_new/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.patches as mpatches\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "from models import *\n",
    "from utils.utils_preprocess import *\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0441f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from utils.utils_train_model import *\n",
    "from utils.utils_preprocess import *\n",
    "from models import *\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "device = (\n",
    "    \"mps\" \n",
    "    if torch.backends.mps.is_available() \n",
    "    else \"cuda\" \n",
    "    if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "data_dict_path = './output_data/data_dict.pth'\n",
    "data_dict = torch.load(data_dict_path)\n",
    "\n",
    "print(\"Processing data\")\n",
    "print([len(data_dict[f'{i}']) for i in range(1, 8)])\n",
    "\n",
    "data_list = sum([data_dict[str(i)] for i in range(1, 8)], [])\n",
    "total_size = len(data_list)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_data, val_data = random_split(data_list, [train_size, val_size])\n",
    "\n",
    "# Step 2: Create DataLoader for training and validation\n",
    "batch_size = 64\n",
    "train_loader = GeoDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = GeoDataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 3: Model setup\n",
    "# Some of the datas don't have adj matrix\n",
    "num_node_features = next(data.x.shape[1] for data in data_list if data.x is not None)\n",
    "\n",
    "hidden_channels = 64\n",
    "model_dict = get_model_list(device, num_node_features, hidden_channels)\n",
    "# model = Net_Alex(num_node_features, hidden_channels).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start training ...')\n",
    "for i, (model_name, model) in enumerate(model_dict.items()):\n",
    "    params = {\n",
    "    \"model\": model_name,\n",
    "    \"num_epochs\": 25,\n",
    "    \"lr\":0.001,\n",
    "    }\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    cumulative_preds = []\n",
    "    cumulative_labels = []\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for batch in train_loader:\n",
    "                batch  = batch.to(device)\n",
    "                x = batch.x\n",
    "                edge_index = batch.edge_index\n",
    "                edge_weight = batch.edge_weight\n",
    "                image_features = batch.image_features\n",
    "                batch_y = batch.y\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 2 == 0:\n",
    "                    out = model(image_features)\n",
    "                    loss = criterion(out, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    _, pred = out.max(dim=1)\n",
    "                    correct += (pred == batch_y).sum().item()\n",
    "                else:\n",
    "                    GNN_output, CNN_output = model(x, edge_index, edge_weight, batch.batch, image_features)\n",
    "                    if GNN_output is not None:\n",
    "                        loss_GNN = criterion(GNN_output, batch_y)\n",
    "                        loss_AlexNet = criterion(CNN_output, batch_y)\n",
    "                        loss = (loss_GNN + loss_AlexNet) / 2\n",
    "                        loss.backward()\n",
    "                        _, pred = GNN_output.max(dim=1)\n",
    "                        correct += (pred == batch_y).sum().item()\n",
    "                    else:\n",
    "                        loss = criterion(CNN_output, batch_y)\n",
    "                        loss.backward()\n",
    "                        _, pred = CNN_output.max(dim=1)\n",
    "                        correct += (pred == batch_y).sum().item()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_accuracy = correct / len(train_data)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            epoch_preds = []\n",
    "            epoch_labels = []\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    x = batch.x\n",
    "                    edge_index = batch.edge_index\n",
    "                    edge_weight = batch.edge_weight\n",
    "                    image_features = batch.image_features\n",
    "                    batch_y = batch.y\n",
    "\n",
    "                    if i % 2 == 0:\n",
    "                        out = model(image_features)\n",
    "                        loss = criterion(out, batch_y)\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                        _, pred = out.max(dim=1)\n",
    "                        val_correct += (pred == batch_y).sum().item()\n",
    "                    else:\n",
    "                        GNN_output, AlexNet_output = model(x, edge_index, edge_weight, batch.batch, image_features)\n",
    "                        if GNN_output is not None:\n",
    "                            loss_GNN = criterion(GNN_output, batch_y)\n",
    "                            loss_AlexNet = criterion(AlexNet_output, batch_y)\n",
    "                            loss = (loss_GNN + loss_AlexNet) / 2\n",
    "                            _, pred = GNN_output.max(dim=1)\n",
    "                            val_correct += (pred == batch_y).sum().item()\n",
    "                        else:\n",
    "                            loss = criterion(AlexNet_output, batch_y)\n",
    "                            _, pred = AlexNet_output.max(dim=1)\n",
    "                            val_correct += (pred == batch_y).sum().item()\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                    epoch_preds.extend(pred.cpu().numpy())\n",
    "                    epoch_labels.extend(batch_y.cpu().numpy())\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = val_correct / len(val_data)\n",
    "            mlflow.log_metric('train_loss', train_loss, step=epoch)\n",
    "            mlflow.log_metric('train_accuracy', train_accuracy, step=epoch)\n",
    "            mlflow.log_metric('val_loss', avg_val_loss, step=epoch)\n",
    "            mlflow.log_metric('val_correct', val_accuracy, step=epoch)\n",
    "            print(f'Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "                f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Accumulate all predictions and labels across epochs\n",
    "            cumulative_preds.extend(epoch_preds)\n",
    "            cumulative_labels.extend(epoch_labels)\n",
    "    \n",
    "    save_path = f'./model/{model_name}.pth'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39810ad",
   "metadata": {},
   "source": [
    "# Grad Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525c67dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: \"Surprised\",\n",
    "    1: \"Fearful\",\n",
    "    2: \"Disgusted\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Sad\",\n",
    "    5: \"Angry\",\n",
    "    6: \"Neutral\"\n",
    "}\n",
    "\n",
    "device = (\n",
    "    \"mps\" \n",
    "    if torch.backends.mps.is_available() \n",
    "    else \"cuda\" \n",
    "    if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dict_path = './output_data/data_dict.pth'\n",
    "data_dict = torch.load(data_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50059d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = sum([data_dict[str(i)] for i in range(1, 8)], [])\n",
    "total_size = len(data_list)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_data, val_data = random_split(data_list, [train_size, val_size])\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "train_loader = GeoDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = GeoDataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_class = 7\n",
    "\n",
    "# Model setup\n",
    "num_node_features = next(data.x.shape[1] for data in data_list if data.x is not None)\n",
    "hidden_channels = 64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Z_GCN_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
